View(y)
iris
head(iris)
plot
install.packages('rpart')
library(rpart)
install.packages('caret')
iibrary(caret)
# 데이터 트레이닝, 테스트데이터 분리
tmp=createDataPartiton(iris$Species,p=0.8)
# 데이터 트레이닝, 테스트데이터 분리
tmp=createDataPartition(iris$Species,p=0.8)
install.packages('caret')
iibrary(caret)
library(caret)
# 데이터 트레이닝, 테스트데이터 분리
tmp=createDataPartition(iris$Species,p=0.8)
str(tmp)
tmp$Resample1
# 데이터 트레이닝, 테스트데이터 분리
tmp=createDataPartition(iris$Species,p=0.8,list=FALSE)
tmp=createDataPartition(iris$Species,p=0.8)
str(tmp)
str(tmp)
#tmp=createDataPartition(iris$Species,p=0.8)
str(rowNum)
(iris$Species,p=0.8,list=FALSE)
# 데이터 트레이닝, 테스트데이터 분리
# 전체 150개의 데이터 중 "80%의 120개를 트레이닝으로 작업
# list=FALSE옵션을 제공하지 않으면 리스트구조로 반환
tmp=createDataPartition(iris$Species,p=0.8,list=FALSE)
#tmp=createDataPartition(iris$Species,p=0.8)
str(rowNum)
# 데이터 트레이닝, 테스트데이터 분리
# 전체 150개의 데이터 중 "80%의 120개를 트레이닝으로 작업
# list=FALSE옵션을 제공하지 않으면 리스트구조로 반환
rowNum=createDataPartition(iris$Species,p=0.8,list=FALSE)
#tmp=createDataPartition(iris$Species,p=0.8)
str(rowNum)
train_data=iris[rowNum,]
str(train_data)
table(train_data)
table(train_data$Species)
test_data=iris[-rowNum,] # -는 제외하고 임.
table(test_data$Species)
iris_rpart=rpart(Species~.,data=train_data, control=rpart.control(minsplit = 2))
# 의사결정트리
install.packages('rpart.plot')
library(rpart.plot)
rpart.plot(iris_rpart)
# 가지치기
# 의사결정 가지를 생성할 때 소용되는 최소 복잡도를 CP(complexity Parament)
str(iris_rpart)
iris_rpart$frame
iris_rpart$cptable
prune(iris_rpart,cp=0.0125)
rpart.plot(iris_rpart)
iris_prune_tree=prune(iris_rpart,cp=0.0125)
rpart.plot(iris_prune_tree)
View(train_data)
# 평가하기(예측하기)
# predict 함수에서 마지막 type='class' 일때는 분류결과를 라벨명으로 출력
# type='prob'일때는 분류결과를 확률로 출력
predict(iris_rpart,test_data, type='class')
predict(iris_rpart,test_data, type='prob')
# 평가하기(예측하기)
# predict 함수에서 마지막 type='class' 일때는 분류결과를 라벨명으로 출력
# type='prob'일때는 분류결과를 확률로 출력
# predict(iris_rpart,test_data, type='prob')
pr=predict(iris_rpart,test_data, type='class')
pr
data.frame(so=test_data$Species,pr)
iris_prdict=data.frame(so=test_data$Species,pr)
table(iris_prdict)
# caret 패키지에서 제공하는 혼돈행렬을 사용할 수도 있음
confusionMatrix(pr, test_data$Species, mode='everything')
# caret 패키지에서 제공하는 혼돈행렬을 사용할 수도 있음
# Error: package e1071 is required시 e1071 패키지 설치
install.packages('e1071')
library(e1071)
confusionMatrix(pr, test_data$Species, mode='everything')
View(iris_prdict)
library(rpart)
library(caret)
apple_df <- read.csv('/Users/dean/py_data/apple.csv', sep="\t", stringsAsFactors=TRUE)
str(apple_df)
library(rpart)
library(caret)
rowNum=createDataPartition(apple_df$Species,p=0.8,list=FALSE)
apple_df <- read.csv('/Users/dean/py_data/apple.csv', sep="\t", stringsAsFactors=TRUE)
apple_df <- read.csv('/Users/dean/py_data/apple.csv', sep="\t", stringsAsFactors=TRUE)
model=c('아오리','홍옥','아오리','미시마','아오리','홍로','로얄후지','아오리','홍로','로얄후지','아오리','홍로','미시마','홍로','홍로','로얄후지','미시마','홍옥','홍옥','홍옥','홍옥','미시마','로얄후지','미시마','로얄후지')
weight=c(286,256,251,396,282,342,407,238,295,392,298,352,409,152,329,394,408,224,211,235,239,380,381,391,384)
sugar=c(12.9,13.4,12.1,16.3,15,13.4,13.6,13.5,15.1,15.6,13.3,14.4,14.2,13.5,13.8,13,14.1,12.1,13.4,14.2,12.3,16.7,14.2,16.8,13.3)
acid=c(0.31,0.69,0.32,0.39,0.29,0.31,0.4,0.31,0.29,0.38,0.32,0.28,0.39,0.32,0.31,0.38,0.39,0.73,0.64,0.65,0.72,0.38,0.4,0.41,0.41)
color=c('홍색','적색','홍색','홍색','홍색','홍색','적색','홍색','홍색','적색','홍색','홍색','홍색','홍색','홍색','적색','홍색','적색','적색','적색','적색','홍색','적색','홍색','적색')
apple_df=data.frame(model,weight,sugar,acid,color)
str(apple_df)
createDataPartition(apple_df$Species,p=0.8,list=FALSE)
createDataPartition(apple_df$Species,p=0.8,list=FALSE)
createDataPartition(apple_df$Species,p=0.8)
# 데이터 트레이닝, 테스트데이터 분리
createDataPartition(apple_df$model,p=0.8,list=FALSE)
# 데이터 트레이닝, 테스트데이터 분리
rowNum=createDataPartition(apple_df$model,p=0.8,list=FALSE)
str(rowNum)
train_data=apple_df[rowNum,]
str(train_data)
table(train_data$model)
test_data=apple_df[-rowNum,]
table(test_data$model)
# rpart를 이용한 분류분석
rpart(model~.,data=train_data, control=rpart.control(minsplit = 2))
# rpart를 이용한 분류분석
apple_rpart=rpart(model~.,data=train_data, control=rpart.control(minsplit = 2))
# 의사결정트리
library(rpart.plot)
rpart.plot(apple_rpart)
# 가지치기(prune) : depth를 줄이는 과정
str(apple_rpart)
apple_rpart$cptable
prune(apple_rpart,cp=0.0625)
apple_prune_tree=prune(apple_rpart,cp=0.0625)
rpart.plot(apple_prune_tree)
# 평가하기(예측하기)
pr=predict(apple_rpart,test_data, type='class')
pr
# 평가하기(예측하기)
pr=predict(apple_rpart,test_data, type='prob')
pr
# 평가하기(예측하기)
pr=predict(apple_rpart,test_data, type='class')
pr
apple_prdict=data.frame(so=test_data$model,pr)
table(apple_prdict)
# 혼돈행렬
library(e1071)
confusionMatrix(pr, test_data$model, mode='everything')
View(apple_rpart)
apple_prune_tree=prune(apple_rpart,cp=cp[2,1])
cp=apple_rpart$cptable
prune
rpart.plot(apple_prune_tree)
apple_prune_tree=prune(apple_rpart,cp=cp[3,1])
rpart.plot(apple_prune_tree)
# 한글깨짐현상 해결
install.packages("extrafont")
library(extrafont)
font_import()
apple_df
in=c('3.5')
new=c('300','15','0.5','홍색')
str(apple_df)
홍색사과=apple_df[apple_df$color=='홍색', 2:4]
적색사과=apple_df[apple_df$color=='적색색', 2:4]
plot(홍색사과)
홍색2=홍색사과[홍색사과$acid<0.36 & 홍색사과$weight<350,]
plot(홍색2)
View(tmp)
apple_new_model=apple_df[,2:5]
str(apple_new_model)
summary(apple_new_model)
install.packages('clustMixType')
library(clustMixType)
kproto(apple_new_model,2)
apple_kproto=kproto(apple_new_model,2)
str(apple_kproto)
apple_kproto$cluster
#for를 이용한 군집갯수 변화
re=0
for(i in 1:10){
apple_kproto=kproto(apple_new_model,i)  #2개의 군집으로 나누자
re[i]=apple_kproto$tot.withinss #군집내 편차제곱의 합
}
re
par(mar=c(5,5,5,5))
plot(re,type='b')  # 선과 점을 동시에 그림
# 4개의 군집선을 그림
apple_cluster=kproto(apple_new_model,4)
apple_cluster
str(apple_cluster)
table(apple_cluster$cluster)
apple_cluste$cluster #클러스터로 나뉜 사과품종
apple_cluster$cluster #클러스터로 나뉜 사과품종
apple_df$model #실제 자료
table(apple_cluster$cluster, apple_df$model)
# 항목별 군집간 차이 확인
clprofiles(apple_cluster,apple_new_model)
apple_kproto$tot.withinss # 군집
# 클리스터링 : https://brunch.co.kr/@jakeyim/26
# for를 이용한 군집갯수 변화
# kproto 함수는 자체적으로 표준화
re=0
for(i in 1:10){
apple_kproto=kproto(apple_new_model,i)  #2개의 군집으로 나누자
re[i]=apple_kproto$tot.withinss #군집내 편차제곱의 합
}
re
par(mar=c(5,5,5,5))
plot(re,type='b')  # 선과 점을 동시에 그림
# 4개의 군집선을 그림
apple_cluster=kproto(apple_new_model,4)
apple_cluster
str(apple_cluster)
table(apple_cluster$cluster)
apple_cluster$cluster #클러스터로 나뉜 사과품종
apple_df$model #실제 자료
table(apple_cluster$cluster, apple_df$model)
# 항목별 군집간 차이 확인
clprofiles(apple_cluster,apple_new_model)
# 클러스터별 자료
re=apple_cluster$cluster
group_4=apple_df[grep(4,re),]
group_3=apple_df[grep(3,re),]
group_2=apple_df[grep(2,re),]
group_1=apple_df[grep(1,re),]
group_1
group_2
plot(re,type='b')  # 선과 점을 동시에 그림
txt <- c( "동남아,푸켓,수영복,유심,패키지,가족여행,자유여행,리조트,베트남",
"가족여행,패키지,유럽,푸켓,자유여행,환율,신혼여행,신발사이즈표",
"보라카이,신혼여행,날씨,환율,비행기표,풀빌라,시차",
"패키지,동남아,가족여행,휴양지,여행지추천,특가",
"일본,번역기,후쿠오카,온천,가족여행,리조트,포켓와이파이",
"몰디브,신혼여행,항공권,동남아,비행시간,숙소,비자,발리,풀빌라",
"호텔,동남아,세부,호핑투어,리조트,신혼여행,풀빌라,필리핀",
"푸켓,풀빌라,여행,신혼여행,자유여행,와이파이,코타키나발루",
"동남아,보홀,보라카이,팔라완,가족여행,스쿠버다이빙,여행책",
"푸켓,가족여행,보라카이,동남아,스쿠버다이빙,리조트,피피섬",
"배낭여행,유럽,호스텔,북유럽,서유럽,파리,루브르,에투알,에펠",
"이탈리아,베네치아,토스카니,피렌체,바티칸,여행지도",
"하와이,괌,푸켓,세부,리조트,가족여행,골드카드",
"괌,푸켓,세부,호텔,풀빌라,가족여행,힐튼,쉐라톤",
"베네치아,피렌체,신혼여행,로마,패키지",
"배낭여행,유럽,호텔팩,공항,환율,픽업서비스,런던,파리,체코,호스텔",
"특가,파리,환율,스위스,이탈리아,오스트리아,와이파이,호스텔",
"지중해,유럽,특가,배낭여행,패키지,파리,스위스,이탈리아,오스트리아",
"유럽,동유럽,날씨,체코,환율,비엔나,배낭여행,부다페스트,호스텔",
"유심,체코,신혼여행,크로아티아,패키지,비엔나,류블랴냐,독일,동유럽,부다페스트",
"패키지,지중해,호텔,유럽,동유럽,폴란드,부다페스트,신혼여행,프라하,크로아티아",
"동유럽,폴란드,체코,프라하,독일,크로아티아,날씨",
"이스탄불,호스텔,유럽,자유여행,배낭여행,지중해,날씨,파묵칼레",
"신혼여행,이탈리아,지중해,날씨,유럽,자유여행,와이파이,유심",
"이탈리아,지중해,산토리니,아테네,유럽,터키",
"유심,터키,유럽,그리스,지중해,이탈리아",
"배낭여행,유심,지중해,아테네,산토리니,메테오라,로마,베네치아",
"유럽,날씨,동유럽,사진,우산,3박10일,패키지")
str(txt)
# 구분자를 기반으로 자료 분리, 리스트 구조로 분리
txt_split=strsplit(txt,",")
txt_split
str(txt_split)
txt_split[[28][3]] # 리스트구조의 인덱싱
txt_split[[28]][[[3]] # 리스트구조의 인덱싱
txt_split[[28]][[3]] # 리스트구조의 인덱싱
txt_unlist=unlist(txt_split)
txt_unlist
txt_unlist=unlist(txt_split)
txt_unlist
txt_table=table(txt_unlist)
txt_table
txt_sort=sort(txt_table,decreasing=T)
txt_top7=txt_sort[1:7]
txt_top7
barplot(txt_top7)
barplot(txt_top7)
install.packages('wordcloud')
library(wordcloud)
par(mar=c(5,5,5,5))
wordcloud(names(txt_table),freq=txt_table, random.order=F,
min.freq=5, rot.per=0, scale=c(3,1))
find=txt[grep("유럽",txt_split)]
find
# 한글 깨짐 해결
library(ggplot2) theme_set(theme_grey(base_family='NanumGothic'))
# 한글 깨짐 해결
library(ggplot2)
theme_set(theme_grey(base_family='NanumGothic'))
barplot(txt_top7)
install.packages('wordcloud')
install.packages("wordcloud")
par(mar=c(5,5,5,5))
wordcloud(names(txt_table),freq=txt_table, random.order=F,
min.freq=5, rot.per=0, scale=c(3,1))
wordcloud(names(txt_table),freq=txt_table, random.order=F,
min.freq=5, rot.per=0, scale=c(3,1))
find=txt[grep("유럽",txt_split)]
find
# 연관분석
install.packages('arules')
# 한글 깨짐 해결
library(ggplot2)
theme_set(theme_grey(base_family='NanumGothic'))
barplot(txt_top7)
# 빅카인즈 자료에서는 필요한 열(기사분석된)을 복사해서
# 메모창에 붙여넣기 한 후에
#
# 한글 깨지는 것 해결
install.packages('extrafont')
install.packages("extrafont")
library('extrafont')
font_import() #맥에 설치된 폰트를 R로 가져옴.
library('extrafont')
font_import() #맥에 설치된 폰트를 R로 가져옴.
fonts() #가져온 폰트목록
Sys.setlocale('LC_ALL' , 'ko_KR.UTF-8')
sessionInfo()
txt <- c( "동남아,푸켓,수영복,유심,패키지,가족여행,자유여행,리조트,베트남",
"가족여행,패키지,유럽,푸켓,자유여행,환율,신혼여행,신발사이즈표",
"보라카이,신혼여행,날씨,환율,비행기표,풀빌라,시차",
"패키지,동남아,가족여행,휴양지,여행지추천,특가",
"일본,번역기,후쿠오카,온천,가족여행,리조트,포켓와이파이",
"몰디브,신혼여행,항공권,동남아,비행시간,숙소,비자,발리,풀빌라",
"호텔,동남아,세부,호핑투어,리조트,신혼여행,풀빌라,필리핀",
"푸켓,풀빌라,여행,신혼여행,자유여행,와이파이,코타키나발루",
"동남아,보홀,보라카이,팔라완,가족여행,스쿠버다이빙,여행책",
"푸켓,가족여행,보라카이,동남아,스쿠버다이빙,리조트,피피섬",
"배낭여행,유럽,호스텔,북유럽,서유럽,파리,루브르,에투알,에펠",
"이탈리아,베네치아,토스카니,피렌체,바티칸,여행지도",
"하와이,괌,푸켓,세부,리조트,가족여행,골드카드",
"괌,푸켓,세부,호텔,풀빌라,가족여행,힐튼,쉐라톤",
"베네치아,피렌체,신혼여행,로마,패키지",
"배낭여행,유럽,호텔팩,공항,환율,픽업서비스,런던,파리,체코,호스텔",
"특가,파리,환율,스위스,이탈리아,오스트리아,와이파이,호스텔",
"지중해,유럽,특가,배낭여행,패키지,파리,스위스,이탈리아,오스트리아",
"유럽,동유럽,날씨,체코,환율,비엔나,배낭여행,부다페스트,호스텔",
"유심,체코,신혼여행,크로아티아,패키지,비엔나,류블랴냐,독일,동유럽,부다페스트",
"패키지,지중해,호텔,유럽,동유럽,폴란드,부다페스트,신혼여행,프라하,크로아티아",
"동유럽,폴란드,체코,프라하,독일,크로아티아,날씨",
"이스탄불,호스텔,유럽,자유여행,배낭여행,지중해,날씨,파묵칼레",
"신혼여행,이탈리아,지중해,날씨,유럽,자유여행,와이파이,유심",
"이탈리아,지중해,산토리니,아테네,유럽,터키",
"유심,터키,유럽,그리스,지중해,이탈리아",
"배낭여행,유심,지중해,아테네,산토리니,메테오라,로마,베네치아",
"유럽,날씨,동유럽,사진,우산,3박10일,패키지")
str(txt)
# 구분자를 기반으로 자료 분리, 리스트 구조로 분리
txt_split=strsplit(txt,",")
txt_split
str(txt_split)
txt_split[[28]][[3]] # 리스트구조의 인덱싱
txt_unlist=unlist(txt_split)
txt_unlist
txt_table=table(txt_unlist)
txt_table
txt_sort=sort(txt_table,decreasing=T)
txt_top7=txt_sort[1:7]
txt_top7
barplot(txt_top7)
barplot(txt_top7)
barplot(txt_top7)
install.packages('wordcloud')
library(wordcloud)
par(mar=c(5,5,5,5))
wordcloud(names(txt_table),freq=txt_table, random.order=F,
min.freq=5, rot.per=0, scale=c(3,1))
find=txt[grep("유럽",txt_split)]
find
library(arules)
# 원본자료를 strsplit(자료, 나누는 기준)으로 나누어서 list로 변경후
txt_transaction=as(txt_split,'transactions')
txt_transaction
inspect(txt_transaction)
rules=apriori(txt_transaction,parameter=list(support=0.1,confidence=0.8))
rules
inspect(rules)
re=subset(rules,subset=lift>1.2) # 지지도는 1이 기본임
re
re_sort=inspect(sort(re,decreasing=T,by='lift'))
str(re_sort)
re@lhs
install.packages('arulesViz')
library(arulesViz)
plot(rules, method="graph", control=list(type="items"))
plot(rules, method="graph", control=list(type="group"))
plot(rules, method="group", control=list(type="items"))
par(family="NanumGothic")
plot(rules, method="graph", control=list(type="items"))
plot(rules, method="graph", control=list(type="group"))
plot(rules, method="group", control=list(type="items"))
plot(rules, method="group", control=list(type="items"))
wordcloud(names(txt_table),freq=txt_table, random.order=F,
min.freq=5, rot.per=0, scale=c(3,1))
install.packages('shiny')
library(shiny)
runExample()
runExample('01_hello')
runExample('01_text')
runExample('01_text')
runExample('02_text')
runExample('03_reactivity')
str(runExample)
str(runExample())
runExample('04_mpg')
runExample('05_sliders')
runExample('08_html')
runApp('~/basic_plot')
runApp('~/basic_plot')
# 샤이니 웹서버와 연결
install.packages('rsconnect')
(rsconnect)
(rsconnect)
# 샤이니 웹서버와 연결
install.packages('rsconnect')
library(rsconnect)
rsconnect::setAccountInfo(name='dean99',
token='439D754A3D8C355A4DCC6F2A2FE60703',
secret='GdAGUGTuUYLZoKv7mzhqXMAcr9IxPvUcinqevUV6')
# Define UI for application that draws a histogram
ui <- fluidPage(
# Application title
titlePanel("bss-------test chart"),
# Sidebar with a slider input for number of bins
sidebarLayout(
sidebarPanel(
sliderInput("bins",
"Number of bins:",
min = 1,
max = 20,
value = 10)
),
# Show a plot of the generated distribution
mainPanel(
plotOutput("distPlot")
)
)
)
runApp('~/basic_plot')
View(rowNum)
mtcars
plot(mtcars$mpg)
str(mtcars)
runApp('~/basic_plot')
library(shiny)
runApp('~/basic_plot')
str(iris)
runApp('~/basic_plot')
runApp('~/basic_plot')
model=c('아오리','홍옥','아오리','미시마','아오리','홍로','로얄후지','아오리','홍로','로얄후지','아오리','홍로','미시마','홍로','홍로','로얄후지','미시마','홍옥','홍옥','홍옥','홍옥','미시마','로얄후지','미시마','로얄후지')
weight=c(286,256,251,396,282,342,407,238,295,392,298,352,409,152,329,394,408,224,211,235,239,380,381,391,384)
sugar=c(12.9,13.4,12.1,16.3,15,13.4,13.6,13.5,15.1,15.6,13.3,14.4,14.2,13.5,13.8,13,14.1,12.1,13.4,14.2,12.3,16.7,14.2,16.8,13.3)
acid=c(0.31,0.69,0.32,0.39,0.29,0.31,0.4,0.31,0.29,0.38,0.32,0.28,0.39,0.32,0.31,0.38,0.39,0.73,0.64,0.65,0.72,0.38,0.4,0.41,0.41)
color=c('홍색','적색','홍색','홍색','홍색','홍색','적색','홍색','홍색','적색','홍색','홍색','홍색','홍색','홍색','적색','홍색','적색','적색','적색','적색','홍색','적색','홍색','적색')
apple_df=data.frame(model,weight,sugar,acid,color)
str(apple_df)
# 샤이니 앱설치
library(shiny)
runExample()
str(runExample())
runExample('08_html')
runExample('04_mpg')
runExample('03_reactivity')
runExample('01_hello')
runExample('02_text')
runExample('05_sliders')
runExample('06_tabsets')
runExample('07_widgets')
library(httr)
library(rvest)
library(jsonlite)
install.packages('rvest')
library(rvest)
> httr::GET(url   = "https://section.blog.naver.com/ajax/SearchList.nhn",
+           query = list("countPerPage" = "7",
+                        "currentPage"  = 1,
+                        "endDate"      = "2021-03-17",
+                        "keyword"      = "유아썬크림",
+                        "orderBy"      = "sim",
+                        "startDate"    = "2017-01-01",
+                        "type"         = "post"),
+           add_headers("referer" = "https://section.blog.naver.com/Search/Post.nh")) %>%
+   print()
setwd("/Users/dean/R/data")
result=readLines("유아썬크림_검색결과.txt")
result
str(result)
# XML 태그를 공란으로 치환
result_refined <- gsub("<(\\/?)(\\w +)*([^<>]*)>", " ", result)
# 단락을 표현하는 불필요한 문자를 공란으로 치환
result_refined <- gsub("[[:punct:]]", " ", result_refined)
# 영어 소문자를 공란으로 치환
result_refined <- gsub("[a-z]", " ", result_refined)
# 숫자를 공란으로 치환
result_refined <- gsub("[0-9]", " ", result_refined)
# 여러 공란은 한 개의 공란으로 변경
result_refined <- gsub(" +", " ", result_refined)
result_refined
# KoNLP 패키지
library(KoNLP)
# 명사 단어 추출
result_noun = extractNoun(result_refined)
# 상위 50개만 보기
str(result_noun)
head(result_noun, 50)
# 길이가 9이상인 문자를 제외
result_noun_refined = result_noun[nchar(result_noun) > 9]
head(result_noun_refined, 50)
# 단어 수작업으로 뺴주기
nouns_excluded = c("썬크림","유아썬크림","유아","한","아이","베이비","아기",
"들","수","을","추천","선크림","것","우리","은","이","때",
"아기썬크림","물","후","썬크림을","으로","적","저","중","도")
#  'in'을 부정하는 방법으로 인덱싱하여 제외
noun_final = result_noun_refined[!result_noun_refined %in% nouns_excluded]
head(noun_final, 50)
# WordCloud
library(wordcloud2)
# noun_final 객체에 저장되어 있는 명사별 빈도수
str(noun_final)
# table() 함수를 이용하여 이를 정리하고, 빈도수 내림차순 순으로 정렬
word = sort(table(noun_final), decreasing=T)[1:50]
# 그래프 그리기
wordcloud2(data = word, size = 0.4, shape = 'diamond')
